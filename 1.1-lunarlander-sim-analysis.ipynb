{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "mpl.rc('savefig', dpi=300)\n",
    "mpl.rc('text', usetex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_dir = os.path.join('data', 'lunarlander-sim')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Table 1, Figure 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "using_lander_reward_shaping = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reward_logs = {}\n",
    "rew_data_dir = os.path.join(data_dir, '12.12' if using_lander_reward_shaping else '12.22')\n",
    "for f in os.listdir(rew_data_dir):\n",
    "  if (not using_lander_reward_shaping and (f.endswith('-Y.pkl') or f.endswith('-N.pkl'))) or (using_lander_reward_shaping and (f.endswith('-Y.pkl') or f.endswith('-4.pkl') or f.endswith('-3.pkl') or f == 'noop-rawaction.pkl')):\n",
    "    with open(os.path.join(rew_data_dir, f), 'rb') as f:\n",
    "      reward_logs.update(pickle.load(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if using_lander_reward_shaping:\n",
    "  reward_logs = {k: v for k, v in reward_logs.items() if not (('model' in k or 'super' in k) and 'using' not in k)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def moving_avg(d, n=5):\n",
    "    s = np.concatenate((np.zeros(1), np.cumsum(d).astype(float)))\n",
    "    return (s[n:] - s[:-n]) / n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_nan_rew_mat(raw, max_ep_len=800):\n",
    "  X = np.zeros((len(raw), min(max_ep_len, max(len(x) for x in raw))))\n",
    "  X[:, :] = np.nan\n",
    "  for i, x in enumerate(raw):\n",
    "    x = x[:max_ep_len]\n",
    "    X[i, :len(x)] = x\n",
    "  return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_nan_outcome_mat(raw, outcome, max_ep_len=800):\n",
    "  X = np.zeros((len(raw), min(max_ep_len, max(len(x) for x in raw))))\n",
    "  X[:, :] = np.nan\n",
    "  for i, x in enumerate(raw):\n",
    "    x = x[:max_ep_len]\n",
    "    X[i, :len(x)] = [1 if z == outcome else 0 for z in x]\n",
    "  return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "traj_col_means = lambda x: np.nanmean(x, axis=0)\n",
    "traj_col_stderrs = lambda x: np.nanstd(x, axis=0) / np.sqrt(np.count_nonzero(~np.isnan(x), axis=0))\n",
    "r_mins = lambda x: traj_col_means(x) - traj_col_stderrs(x)\n",
    "r_maxs = lambda x: traj_col_means(x) + traj_col_stderrs(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SUCCESS = 100\n",
    "CRASH = -100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if using_lander_reward_shaping:\n",
    "  pilot_evals_path = os.path.join(data_dir, '12.12', 'pilot_evals.pkl')\n",
    "else:\n",
    "  pilot_evals_path = os.path.join(data_dir, '12.22', 'pilot_evals.pkl')\n",
    "with open(pilot_evals_path, 'rb') as f:\n",
    "  pilot_evals = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_config(R, prop_label, color, fill=True, smooth_win=20):\n",
    "  if fill:\n",
    "    x = range(R.shape[1] - (smooth_win - 1))\n",
    "    y1 = moving_avg(r_mins(R), n=smooth_win)\n",
    "    y2 = moving_avg(r_maxs(R), n=smooth_win)\n",
    "    plt.fill_between(x, y1, y2, where=y2 >= y1, interpolate=True, facecolor=color, label=prop_label, alpha=0.5)\n",
    "    plt.plot(moving_avg(traj_col_means(R), n=smooth_win), color)\n",
    "  else:\n",
    "    plt.plot(moving_avg(traj_col_means(R), n=smooth_win), color, label=prop_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "normalize = lambda x: x / x.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def outcome_rates(outcomes, outcome):\n",
    "  outcomes = [[1 if x == outcome else 0 for x in y] for y in outcomes]\n",
    "  return make_nan_rew_mat(outcomes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def outcome_rate_of_pilot_eval(outcomes, outcome):\n",
    "  return np.mean([1 if x == outcome else 0 for x in outcomes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def outcome_stderr_of_pilot_eval(outcomes, outcome):\n",
    "  d = [1 if x == outcome else 0 for x in outcomes]\n",
    "  return np.std(d) / np.sqrt(len(d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_reward_logs = {}\n",
    "for k, v in reward_logs.items():\n",
    "  d = eval(k)\n",
    "  tmp_using_lander_reward_shaping = d['using_lander_reward_shaping'] if 'using_lander_reward_shaping' in d else True\n",
    "  new_k = (round(d['pilot_tol'], 2 if d['pilot_type'] == 'sensor' else 1), d['pilot_type'], d['embedding_type'], tmp_using_lander_reward_shaping)\n",
    "  new_reward_logs[new_k] = v\n",
    "reward_logs = new_reward_logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pilot_names = ['full', 'laggy', 'noisy', 'noop', 'sensor']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for pilot_name in pilot_names:\n",
    "  pilot_eval = pilot_evals[pilot_name]\n",
    "  d = {\n",
    "    'rewards': [[x]*100 for x in pilot_eval[0]],\n",
    "    'outcomes': [[x]*100 for x in pilot_eval[1]]\n",
    "  }\n",
    "  for embedding_type in ['rawaction', 'modelbasedgoalinf', 'supervisedgoalinf']:\n",
    "    for tmp_using_lander_reward_shaping in [True, False]:\n",
    "      reward_logs[(1.0, pilot_name, embedding_type, tmp_using_lander_reward_shaping)] = d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_reward_logs = {}\n",
    "for config_name, config_data in reward_logs.items():\n",
    "  d = {}\n",
    "  for k, v in config_data.items():\n",
    "    if k == 'rewards':\n",
    "      d['rewards'] = make_nan_rew_mat(v)\n",
    "    elif k == 'outcomes':\n",
    "      d['success'] = make_nan_outcome_mat(v, SUCCESS)\n",
    "      d['crash'] = make_nan_outcome_mat(v, CRASH)\n",
    "  new_reward_logs[config_name] = d\n",
    "reward_logs = new_reward_logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "alphas = [round(x, 1) for x in np.arange(0, 1.1, 0.1)]\n",
    "sensor_alphas = [round(x, 2) for x in np.arange(0, 0.3, 0.03)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_stats_without_copilot(pilot_name):\n",
    "  pilot_eval = pilot_evals[pilot_name]\n",
    "  R = pilot_eval[0]\n",
    "  mean_rew = np.nanmean(R)\n",
    "  stderr_rew = np.nanstd(R) / np.sqrt(np.count_nonzero(~np.isnan(R)))\n",
    "  succ_rate = outcome_rate_of_pilot_eval(pilot_eval[1], outcome=SUCCESS)\n",
    "  crash_rate = outcome_rate_of_pilot_eval(pilot_eval[1], outcome=CRASH)\n",
    "  return int(mean_rew), int(stderr_rew), succ_rate, crash_rate\n",
    "\n",
    "def summarize_reward_log(reward_log):\n",
    "  R = reward_log['rewards']\n",
    "  rews = moving_avg(traj_col_means(R), n=100)\n",
    "  stderr_rews = moving_avg(traj_col_stderrs(R), n=100)\n",
    "  succ_rates = moving_avg(traj_col_means(reward_log['success']), n=100)\n",
    "  crash_rates = moving_avg(traj_col_means(reward_log['crash']), n=100)\n",
    "  t = max(list(range(len(rews))), key=lambda i: rews[i])\n",
    "  return int(rews[t]), int(stderr_rews[t]), succ_rates[t], crash_rates[t], (100+t)\n",
    "\n",
    "def get_stats_with_copilot(pilot_name, embedding_type='rawaction', tmp_using_lander_reward_shaping=False):\n",
    "  if pilot_name == 'noop':\n",
    "    return summarize_reward_log(reward_logs[(0, pilot_name, embedding_type, tmp_using_lander_reward_shaping)]), 0\n",
    "  else:\n",
    "    my_alphas = sensor_alphas if pilot_name == 'sensor' and (embedding_type != 'rawaction' or not tmp_using_lander_reward_shaping) else alphas\n",
    "    stats_for_all_alpha = [summarize_reward_log(reward_logs[(alpha, pilot_name, embedding_type, tmp_using_lander_reward_shaping)]) for alpha in my_alphas]\n",
    "    return max(list(zip(stats_for_all_alpha, alphas)), key=lambda x: x[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def capitalize(x):\n",
    "  return x[0].upper() + x[1:]\n",
    "\n",
    "def format_pilot_name(x):\n",
    "  return capitalize(x).replace('Full', 'Optimal').replace('Noop', 'None')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(os.path.join(data_dir, 'solo-vs-team-table%s.txt' % ('' if using_lander_reward_shaping else '-no-gds')), 'w') as f:\n",
    "  for pilot_name in ['human', 'noop', 'sensor', 'laggy', 'noisy', 'full']:\n",
    "    without_copilot_stats = get_stats_without_copilot(pilot_name)\n",
    "    if pilot_name in ['human', 'full']:\n",
    "      f.write('%s & $%d \\pm %d$  &  %.3f  & %.3f  &  &  &  &  & \\\\\\\\\\n' % (format_pilot_name(pilot_name),\n",
    "        *without_copilot_stats))\n",
    "    else:\n",
    "      with_copilot_stats, best_alpha = get_stats_with_copilot(pilot_name, tmp_using_lander_reward_shaping=using_lander_reward_shaping)\n",
    "      if pilot_name == 'noop':\n",
    "        f.write('%s &  &  &  & $%d \\pm %d$ & %.3f & %.3f & %d & %.1f \\\\\\\\\\n' % (format_pilot_name(pilot_name),\n",
    "        *with_copilot_stats, best_alpha))\n",
    "      else:\n",
    "        f.write('%s & $%d \\pm %d$  &  %.3f  & %.3f  & $%d \\pm %d$ & %.3f & %.3f & %d & %.1f \\\\\\\\\\n' % (format_pilot_name(pilot_name),\n",
    "          *without_copilot_stats, *with_copilot_stats, best_alpha))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Reward')\n",
    "plt.title('Laggy Synthetic Pilot')\n",
    "\n",
    "plot_config(reward_logs[(0, 'noop', 'rawaction', using_lander_reward_shaping)]['rewards'], 'Solo Copilot', 'teal')\n",
    "plot_config(reward_logs[(0.7, 'laggy', 'rawaction', using_lander_reward_shaping)]['rewards'], r'Pilot + Copilot', 'orange')\n",
    "\n",
    "R = pilot_evals['laggy'][0]\n",
    "mean_rew = np.nanmean(R)\n",
    "stderr_rew = np.nanstd(R) / np.sqrt(np.count_nonzero(~np.isnan(R)))\n",
    "max_eps = 780\n",
    "x = np.array(range(max_eps))\n",
    "y1 = np.ones(max_eps) * (mean_rew - stderr_rew)\n",
    "y2 = np.ones(max_eps) * (mean_rew + stderr_rew)\n",
    "plt.fill_between(x, y1, y2, where=y2 >= y1, interpolate=True, facecolor='gray', label='Solo Pilot', alpha=0.5)\n",
    "\n",
    "plt.legend(loc='lower right')\n",
    "if not using_lander_reward_shaping:\n",
    "  plt.xlim([0, 500])\n",
    "plt.savefig(os.path.join(data_dir, 'laggy-pilot-solo-vs-team-reward%s.pdf' % ('' if using_lander_reward_shaping else '-no-gds')), bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Success Rate')\n",
    "plt.title('Laggy Synthetic Pilot')\n",
    "\n",
    "plot_config(reward_logs[(0, 'noop', 'rawaction', using_lander_reward_shaping)]['success'], 'Solo Copilot', 'teal', fill=False)\n",
    "plot_config(reward_logs[(0.7, 'laggy', 'rawaction', using_lander_reward_shaping)]['success'], r'Pilot + Copilot', 'orange', fill=False)\n",
    "\n",
    "R = pilot_evals['laggy'][1]\n",
    "mean_rew = outcome_rate_of_pilot_eval(R, outcome=SUCCESS)\n",
    "plt.axhline(y=mean_rew, linestyle='--', label='Solo Pilot', color='gray')\n",
    "\n",
    "plt.legend(loc='lower left')\n",
    "if not using_lander_reward_shaping:\n",
    "  plt.xlim([0, 500])\n",
    "plt.savefig(os.path.join(data_dir, 'laggy-pilot-solo-vs-team-success-rate%s.pdf' % ('' if using_lander_reward_shaping else '-no-gds')), bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "re-run the cells above with `using_lander_reward_shaping = True`, then run the cells below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "embedding_type = 'rawaction'\n",
    "plt.xlabel(r'Pilot Tolerance $\\alpha$')\n",
    "plt.ylabel('Success Rate')\n",
    "for pilot_type, linestyle, marker in [('laggy', '-', 'o'), ('noisy', ':', 'o'), ('sensor', '--', 'o')]:\n",
    "  my_alphas = sensor_alphas if pilot_type == 'sensor' and not using_lander_reward_shaping else alphas\n",
    "  mean_rews, stderr_rews, succ_rates, crash_rates, time_to_best_perfs = zip(*[summarize_reward_log(reward_logs[(alpha, pilot_type, embedding_type, using_lander_reward_shaping)]) for alpha in my_alphas])\n",
    "  plt.plot(my_alphas, succ_rates, label=capitalize(pilot_type), color='orange', marker=marker, linestyle=linestyle)\n",
    "plt.axhline(y=summarize_reward_log(reward_logs[(0, 'noop', 'rawaction', using_lander_reward_shaping)])[2], linestyle='--', color='teal', label='None')\n",
    "plt.legend(loc='best')\n",
    "plt.savefig(os.path.join(data_dir, 'pilot-vs-alpha-succ-rate%s.pdf' % ('' if using_lander_reward_shaping else '-no-gds')), bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def format_embedding_name(x):\n",
    "  if x == 'rawaction':\n",
    "    return 'Raw Action Embedding'\n",
    "  elif x == 'modelbasedgoalinf':\n",
    "    return 'Bayesian Goal Inference'\n",
    "  elif x == 'supervisedgoalinf':\n",
    "    return 'Supervised Goal Prediction'\n",
    "  else:\n",
    "    raise ValueError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "odd = lambda l: l[:1] + l[1::2] + l[-1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for pilot_type in ['laggy', 'sensor']:\n",
    "  plt.title('%s Synthetic Pilot' % format_pilot_name(pilot_type))\n",
    "  plt.xlabel(r'Pilot Tolerance $\\alpha$')\n",
    "  plt.ylabel('Success Rate')\n",
    "  for embedding_type, color in [('rawaction', 'orange'), ('supervisedgoalinf', 'teal'), ('modelbasedgoalinf', 'gray')]:\n",
    "    if pilot_type == 'sensor' and (embedding_type != 'rawaction' or not using_lander_reward_shaping):\n",
    "      my_alphas = [round(float(x), 2) for x in np.arange(0, 0.3, 0.03)]\n",
    "    else:\n",
    "      my_alphas = alphas\n",
    "    # something's wrong with the first config used at the beginning of each AWS script\n",
    "    # so filter those out\n",
    "    my_alphas = odd(my_alphas)\n",
    "    mean_rews, stderr_rews, succ_rates, crash_rates, time_to_best_perfs = zip(*[summarize_reward_log(reward_logs[(alpha, pilot_type, embedding_type, using_lander_reward_shaping)]) for alpha in my_alphas])\n",
    "    plt.plot(my_alphas, succ_rates, color=color, label=format_embedding_name(embedding_type), marker='o')\n",
    "  plt.legend(loc='best')\n",
    "  plt.savefig(os.path.join(data_dir, '%s-pilot-embedding-vs-alpha%s.pdf' % (pilot_type, '' if using_lander_reward_shaping else '-no-gds')), bbox_inches='tight')\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Table 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(data_dir, 'cross_evals.pkl'), 'rb') as f:\n",
    "  cross_evals = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pilot_ids = ['noop', 'sensor', 'laggy', 'noisy']\n",
    "pilot_names = ['None', 'Sensor', 'Laggy', 'Noisy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_eval_rew = np.zeros((len(pilot_ids), len(pilot_ids)))\n",
    "cross_eval_succ = np.zeros((len(pilot_ids), len(pilot_ids)))\n",
    "cross_eval_crash = np.zeros((len(pilot_ids), len(pilot_ids)))\n",
    "for i, train_pilot_id in enumerate(pilot_ids):\n",
    "  for j, eval_pilot_id in enumerate(pilot_ids):\n",
    "    rewards, outcomes = list(zip(*cross_evals[(train_pilot_id, eval_pilot_id)]))\n",
    "    succ_rate = np.mean([1 if x == 100 else 0 for x in outcomes])\n",
    "    crash_rate = np.mean([1 if x == -100 else 0 for x in outcomes])\n",
    "    mean_rew = np.mean(rewards)\n",
    "    cross_eval_rew[i, j] = mean_rew\n",
    "    cross_eval_succ[i, j] = succ_rate\n",
    "    cross_eval_crash[i, j] = crash_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, pilot_name in enumerate(pilot_names):\n",
    "  print('%s & %d & %d & %d & %d \\\\\\\\' % (pilot_name, *list(cross_eval_rew[i].astype(int))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, pilot_name in enumerate(pilot_names):\n",
    "  print('%s & %0.2f & %0.2f & %0.2f & %0.2f \\\\\\\\' % (pilot_name, *list(cross_eval_succ[i])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, pilot_name in enumerate(pilot_names):\n",
    "  print('%s & %0.2f & %0.2f & %0.2f & %0.2f \\\\\\\\' % (pilot_name, *list(cross_eval_crash[i])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "interleave = lambda x, y: sum(map(list, list(zip(x, y))), [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, pilot_name in enumerate(pilot_names):\n",
    "  print(('%s & %0.2f / %0.2f & %0.2f / %0.2f & %0.2f / %0.2f & %0.2f / %0.2f \\\\\\\\' % (pilot_name, *interleave(list(cross_eval_succ[i]), list(cross_eval_crash[i])))).replace('0.', '.'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
